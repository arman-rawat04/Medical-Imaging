{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7197b0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-08T14:31:58.539148Z",
     "iopub.status.busy": "2025-05-08T14:31:58.538862Z",
     "iopub.status.idle": "2025-05-08T14:32:09.135914Z",
     "shell.execute_reply": "2025-05-08T14:32:09.135345Z"
    },
    "papermill": {
     "duration": 10.603009,
     "end_time": "2025-05-08T14:32:09.137234",
     "exception": false,
     "start_time": "2025-05-08T14:31:58.534225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data handling and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Video processing\n",
    "import cv2\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Utility\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8a70ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:32:09.144123Z",
     "iopub.status.busy": "2025-05-08T14:32:09.143810Z",
     "iopub.status.idle": "2025-05-08T14:32:09.214480Z",
     "shell.execute_reply": "2025-05-08T14:32:09.213705Z"
    },
    "papermill": {
     "duration": 0.075142,
     "end_time": "2025-05-08T14:32:09.215614",
     "exception": false,
     "start_time": "2025-05-08T14:32:09.140472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83a805",
   "metadata": {
    "papermill": {
     "duration": 0.002602,
     "end_time": "2025-05-08T14:32:09.221209",
     "exception": false,
     "start_time": "2025-05-08T14:32:09.218607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing_functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a7de14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:32:09.227556Z",
     "iopub.status.busy": "2025-05-08T14:32:09.227190Z",
     "iopub.status.idle": "2025-05-08T14:32:09.233857Z",
     "shell.execute_reply": "2025-05-08T14:32:09.233328Z"
    },
    "papermill": {
     "duration": 0.010993,
     "end_time": "2025-05-08T14:32:09.234884",
     "exception": false,
     "start_time": "2025-05-08T14:32:09.223891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def overlay_mask_on_frame(original_frame, mask, alpha=0.5):\n",
    "    \"\"\"Overlay mask on the original frame.\"\"\"\n",
    "    # Ensure the mask is single channel (grayscale)\n",
    "    if len(mask.shape) == 2:\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)  # Convert to 3 channels\n",
    "\n",
    "    # Ensure original_frame is in BGR (if not already)\n",
    "    if len(original_frame.shape) == 2:\n",
    "        original_frame = cv2.cvtColor(original_frame, cv2.COLOR_GRAY2BGR)  # Convert to BGR if grayscale\n",
    "\n",
    "    # Resize the mask to match the size of the original frame\n",
    "    mask_resized = cv2.resize(mask, (original_frame.shape[1], original_frame.shape[0]))\n",
    "\n",
    "    # Overlay the mask on the original frame using alpha blending\n",
    "    overlayed_image = cv2.addWeighted(original_frame, 1 - alpha, mask_resized, alpha, 0)\n",
    "\n",
    "    return overlayed_image\n",
    "\n",
    "\n",
    "def apply_clahe(frame):\n",
    "    \"\"\"Apply CLAHE to enhance contrast on a BGR frame.\"\"\"\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced\n",
    "\n",
    "def preprocess_frame_for_model(frame, size=(224, 224)):\n",
    "    \"\"\"Resize and convert a frame to grayscale for model input.\"\"\"\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(frame_gray, size)  # Shape: [H, W]\n",
    "    tensor = torch.from_numpy(resized).float().unsqueeze(0) / 255.0  # Shape: [1, H, W]\n",
    "    return tensor.unsqueeze(0)  # Shape: [1, 1, H, W]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c62a61",
   "metadata": {
    "papermill": {
     "duration": 0.002462,
     "end_time": "2025-05-08T14:32:09.241204",
     "exception": false,
     "start_time": "2025-05-08T14:32:09.238742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Attention Unet Define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec01080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:32:09.247446Z",
     "iopub.status.busy": "2025-05-08T14:32:09.247188Z",
     "iopub.status.idle": "2025-05-08T14:32:09.261788Z",
     "shell.execute_reply": "2025-05-08T14:32:09.261304Z"
    },
    "papermill": {
     "duration": 0.019054,
     "end_time": "2025-05-08T14:32:09.262852",
     "exception": false,
     "start_time": "2025-05-08T14:32:09.243798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        # number of input channels is a number of filters in the previous layer\n",
    "        # number of output channels is a number of filters in the current layer\n",
    "        # \"same\" convolutions\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with learnable parameters\"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, n_coefficients):\n",
    "        \"\"\"\n",
    "        :param F_g: number of feature maps (channels) in previous layer\n",
    "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
    "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, gate, skip_connection):\n",
    "        \"\"\"\n",
    "        :param gate: gating signal from previous layer\n",
    "        :param skip_connection: activation from corresponding encoder layer\n",
    "        :return: output activations\n",
    "        \"\"\"\n",
    "        g1 = self.W_gate(gate)\n",
    "        x1 = self.W_x(skip_connection)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = skip_connection * psi\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_ch=1, output_ch=1):  # Changed img_ch to 1\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = ConvBlock(img_ch, 64)\n",
    "        self.Conv2 = ConvBlock(64, 128)\n",
    "        self.Conv3 = ConvBlock(128, 256)\n",
    "        self.Conv4 = ConvBlock(256, 512)\n",
    "        self.Conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        self.Up5 = UpConv(1024, 512)\n",
    "        self.Att5 = AttentionBlock(F_g=512, F_l=512, n_coefficients=256)\n",
    "        self.UpConv5 = ConvBlock(1024, 512)\n",
    "\n",
    "        self.Up4 = UpConv(512, 256)\n",
    "        self.Att4 = AttentionBlock(F_g=256, F_l=256, n_coefficients=128)\n",
    "        self.UpConv4 = ConvBlock(512, 256)\n",
    "\n",
    "        self.Up3 = UpConv(256, 128)\n",
    "        self.Att3 = AttentionBlock(F_g=128, F_l=128, n_coefficients=64)\n",
    "        self.UpConv3 = ConvBlock(256, 128)\n",
    "\n",
    "        self.Up2 = UpConv(128, 64)\n",
    "        self.Att2 = AttentionBlock(F_g=64, F_l=64, n_coefficients=32)\n",
    "        self.UpConv2 = ConvBlock(128, 64)\n",
    "\n",
    "        self.Conv = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.MaxPool(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.MaxPool(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.MaxPool(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.MaxPool(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "        s4 = self.Att5(gate=d5, skip_connection=e4)\n",
    "        d5 = torch.cat((s4, d5), dim=1)\n",
    "        d5 = self.UpConv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        s3 = self.Att4(gate=d4, skip_connection=e3)\n",
    "        d4 = torch.cat((s3, d4), dim=1)\n",
    "        d4 = self.UpConv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        s2 = self.Att3(gate=d3, skip_connection=e2)\n",
    "        d3 = torch.cat((s2, d3), dim=1)\n",
    "        d3 = self.UpConv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        s1 = self.Att2(gate=d2, skip_connection=e1)\n",
    "        d2 = torch.cat((s1, d2), dim=1)\n",
    "        d2 = self.UpConv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c07293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:32:09.268951Z",
     "iopub.status.busy": "2025-05-08T14:32:09.268751Z",
     "iopub.status.idle": "2025-05-08T14:32:11.574485Z",
     "shell.execute_reply": "2025-05-08T14:32:11.573623Z"
    },
    "papermill": {
     "duration": 2.310028,
     "end_time": "2025-05-08T14:32:11.575610",
     "exception": false,
     "start_time": "2025-05-08T14:32:09.265582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3971176255.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation model weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the model class (ensure the model code is in scope or imported)\n",
    "segmentation_model = AttentionUNet(img_ch=1, output_ch=1)\n",
    "\n",
    "\n",
    "\n",
    "weights_path = '/kaggle/input/seg2/other/default/1/checkpoint_epoch_30.pth'  # Update with correct path\n",
    "\n",
    "state_dict = torch.load(weights_path, map_location=device)\n",
    "\n",
    "segmentation_model.load_state_dict(state_dict)\n",
    "segmentation_model = segmentation_model.to(device)\n",
    "\n",
    "# Set to evaluation mode\n",
    "segmentation_model.eval()\n",
    "\n",
    "print(\"Segmentation model weights loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ff8eaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:32:11.582817Z",
     "iopub.status.busy": "2025-05-08T14:32:11.582364Z",
     "iopub.status.idle": "2025-05-08T14:32:11.604840Z",
     "shell.execute_reply": "2025-05-08T14:32:11.603954Z"
    },
    "papermill": {
     "duration": 0.026955,
     "end_time": "2025-05-08T14:32:11.605807",
     "exception": true,
     "start_time": "2025-05-08T14:32:11.578852",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_mask_matrix_from_video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/4185465501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A2C/ES000105_CH2_1.avi'\u001b[0m  \u001b[0;31m# update this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmask_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_mask_matrix_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should be (num_frames, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_mask_matrix_from_video' is not defined"
     ]
    }
   ],
   "source": [
    "video_path = '/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A2C/ES000105_CH2_1.avi'  # update this\n",
    "mask_matrix = extract_mask_matrix_from_video(video_path, segmentation_model, device)\n",
    "print(mask_matrix.shape)  # Should be (num_frames, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901e5e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:12:58.936798Z",
     "iopub.status.busy": "2025-05-08T12:12:58.936288Z",
     "iopub.status.idle": "2025-05-08T12:12:59.633297Z",
     "shell.execute_reply": "2025-05-08T12:12:59.632642Z",
     "shell.execute_reply.started": "2025-05-08T12:12:58.936773Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mash_mastrix1=extract_mask_matrix_from_video(video_path, segmentation_model, device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b00949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:17:25.093618Z",
     "iopub.status.busy": "2025-05-08T12:17:25.092945Z",
     "iopub.status.idle": "2025-05-08T12:17:25.108104Z",
     "shell.execute_reply": "2025-05-08T12:17:25.107514Z",
     "shell.execute_reply.started": "2025-05-08T12:17:25.093596Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def color_segments_with_skip(coords_sorted, color_mask, colors):\n",
    "    n_segments = len(colors)\n",
    "    total_length = 0\n",
    "    distances = [0]\n",
    "\n",
    "    for i in range(1, len(coords_sorted)):\n",
    "        prev = coords_sorted[i - 1]\n",
    "        curr = coords_sorted[i]\n",
    "        total_length += np.linalg.norm(curr - prev)\n",
    "        distances.append(total_length)\n",
    "\n",
    "    skip_len = 1.5 * total_length / 7\n",
    "    usable_len = total_length - skip_len\n",
    "    segment_len = usable_len / n_segments\n",
    "\n",
    "    start_idx = 0\n",
    "    while start_idx < len(distances) and distances[start_idx] < skip_len:\n",
    "        start_idx += 1\n",
    "\n",
    "    segments = [[] for _ in range(n_segments)]\n",
    "    current_length = distances[start_idx]\n",
    "    i = start_idx\n",
    "    while i < len(distances) and current_length < total_length:\n",
    "        segment_index = int((current_length - skip_len) // segment_len)\n",
    "        if segment_index < n_segments:\n",
    "            segments[segment_index].append(coords_sorted[i])\n",
    "        current_length = distances[i]\n",
    "        i += 1\n",
    "\n",
    "    for seg_id, segment in enumerate(segments):\n",
    "        for x, y in segment:\n",
    "            color_mask[y, x] = colors[seg_id]\n",
    "\n",
    "def get_segment_midpoints(color_mask, segment_colors):\n",
    "    midpoints = []\n",
    "\n",
    "    for color in segment_colors:\n",
    "        color_arr = np.array(color, dtype=np.uint8)\n",
    "        color_pixels = np.all(color_mask == color_arr, axis=-1)\n",
    "\n",
    "        ys, xs = np.where(color_pixels)\n",
    "        if len(xs) == 0 or len(ys) == 0:\n",
    "            midpoints.append(None)\n",
    "            continue\n",
    "\n",
    "        x_mean = int(np.mean(xs))\n",
    "        y_mean = int(np.mean(ys))\n",
    "        midpoints.append((x_mean, y_mean))\n",
    "\n",
    "    return midpoints\n",
    "\n",
    "def get_segment_areas(color_mask, segment_colors):\n",
    "    areas = []\n",
    "\n",
    "    for color in segment_colors:\n",
    "        color_arr = np.array(color, dtype=np.uint8)\n",
    "        color_pixels = np.all(color_mask == color_arr, axis=-1)\n",
    "\n",
    "        ys, xs = np.where(color_pixels)\n",
    "        area = len(xs)  # Area is the number of pixels of this color\n",
    "        areas.append(area)\n",
    "\n",
    "    return areas\n",
    "\n",
    "\n",
    "def extract_colored_inner_lining(color_mask):\n",
    "    left_colors = [\n",
    "        [255, 0, 0],     \n",
    "        [0, 255, 0],     # Green\n",
    "        [0, 0, 255],     # Red\n",
    "    ]\n",
    "    right_colors = [\n",
    "        [255, 255, 0],  \n",
    "        [255, 0, 255],   \n",
    "        [0, 255, 255],  \n",
    "    ]\n",
    "\n",
    "    inner_lining_mask = np.zeros_like(color_mask)\n",
    "\n",
    "    for i in left_colors:\n",
    "        coord=traverse_right(i, color_mask)\n",
    "        for x, y in coord:\n",
    "            inner_lining_mask[x, y] = i\n",
    "\n",
    "    for i in right_colors:\n",
    "        coord=traverse_left(i, color_mask)\n",
    "        for x, y in coord:\n",
    "            inner_lining_mask[x, y] = i\n",
    "    return inner_lining_mask\n",
    "\n",
    "def color_u_mask(mask):\n",
    "    print(mask.shape)\n",
    "    ys, xs = np.where(mask == 255)\n",
    "    coords = np.column_stack((xs, ys))\n",
    "\n",
    "    topmost_index = np.argmin(ys)\n",
    "    x_center = xs[topmost_index]\n",
    "\n",
    "    left_arm = coords[coords[:, 0] < x_center]\n",
    "    right_arm = coords[coords[:, 0] > x_center]\n",
    "\n",
    "    left_sorted = left_arm[np.argsort(left_arm[:, 1])]\n",
    "    right_sorted = right_arm[np.argsort(right_arm[:, 1])]\n",
    "\n",
    "    left_colors = [\n",
    "        [255, 0, 0],     # Blue\n",
    "        [0, 255, 0],     # Green\n",
    "        [0, 0, 255],     # Red\n",
    "    ]\n",
    "    right_colors = [\n",
    "        [255, 255, 0],   # Cyan\n",
    "        [255, 0, 255],   # Magenta\n",
    "        [0, 255, 255],   # Yellow\n",
    "    ]\n",
    "\n",
    "    color_mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    color_segments_with_skip(left_sorted, color_mask, left_colors)\n",
    "    color_segments_with_skip(right_sorted, color_mask, right_colors)\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    return color_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934b565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:12:59.648933Z",
     "iopub.status.busy": "2025-05-08T12:12:59.648706Z",
     "iopub.status.idle": "2025-05-08T12:12:59.665275Z",
     "shell.execute_reply": "2025-05-08T12:12:59.664548Z",
     "shell.execute_reply.started": "2025-05-08T12:12:59.648917Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def visualize_mask_matrix(mask_matrix, num_frames=5):\n",
    "#     \"\"\"Visualize a few frames from the mask matrix.\"\"\"\n",
    "#     # plt.figure(figsize=(15, 5))\n",
    "#     for i in range(min(num_frames, mask_matrix.shape[0])):\n",
    "        \n",
    "#         plt.imshow(mask_matrix[i])\n",
    "#         plt.show()\n",
    "\n",
    "   \n",
    "# visualize_mask_matrix(mask_matrix, num_frames=20)\n",
    "# visualize_mask_matrix(mash_mastrix1, num_frames=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd84f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:27:34.333571Z",
     "iopub.status.busy": "2025-05-08T12:27:34.332845Z",
     "iopub.status.idle": "2025-05-08T12:27:34.342326Z",
     "shell.execute_reply": "2025-05-08T12:27:34.341565Z",
     "shell.execute_reply.started": "2025-05-08T12:27:34.333545Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "def extract_mask_matrix_from_video(video_path, model, device, frame_size=(224, 224)):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    mask_list = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_clahe = apply_clahe(frame)\n",
    "\n",
    "        input_tensor = preprocess_frame_for_model(frame_clahe, size=frame_size).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_mask = model(input_tensor)  # Shape: [1, 1, H, W]\n",
    "            \n",
    "        binary_mask = (pred_mask.squeeze().cpu().numpy() > 0.5).astype(np.uint8)  # Shape: [H, W]\n",
    "        mask_list.append(binary_mask)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    mask_matrix = np.stack(mask_list, axis=0) \n",
    "    return mask_matrix\n",
    "\n",
    "def plot_image(image):\n",
    "\n",
    "    plt.imshow(image)  # Color image (3 channels)\n",
    "\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "def get_segment_points(color_mask, segment_colors):\n",
    "    segment_points = []\n",
    "\n",
    "    for color in segment_colors:\n",
    "        # print(f\"Checking for color: {color}\")\n",
    "        color = np.array(color, dtype=np.uint8)\n",
    "        for i in range(color_mask.shape[0]):\n",
    "            for j in range(color_mask.shape[1]):\n",
    "                if np.array_equal(color_mask[i][j], color):  # Check if the pixel matches the segment color\n",
    "                    segment_points.append([i, j])\n",
    "                    \n",
    "    return segment_points\n",
    "def apply_clahe(frame):\n",
    "    \"\"\"Apply CLAHE to enhance contrast on a BGR frame.\"\"\"\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    merged = cv2.merge((cl, a, b))\n",
    "    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced\n",
    "\n",
    "def preprocess_frame_for_model(frame, size=(224, 224)):\n",
    "    \"\"\"Resize and convert a frame to grayscale for model input.\"\"\"\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(frame_gray, size)  # Shape: [H, W]\n",
    "    tensor = torch.from_numpy(resized).float().unsqueeze(0) / 255.0  # Shape: [1, H, W]\n",
    "    return tensor.unsqueeze(0)  # Shape: [1, 1, H, W]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75229f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-08T12:41:16.591Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "video_path = \"/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A4C/ES0001 _4CH_1.avi\"\n",
    "video_matrix = Feature_extraction(video_path, segmentation_model, device)\n",
    "\n",
    "print(\"Video shape:\", video_matrix.shape)  # (num_frames, height, width, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f26b99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def traverse_left(colour, mask):\n",
    "    coord = []\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            if np.array_equal(mask[i][j], colour):\n",
    "                coord.append([i, j])\n",
    "                break\n",
    "    return coord\n",
    "\n",
    "def traverse_right(colour, mask):\n",
    "    coord = []\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]-1, -1, -1):\n",
    "            if np.array_equal(mask[i][j], colour):\n",
    "                coord.append([i, j])\n",
    "                break\n",
    "    return coord\n",
    "\n",
    "def traverse_up(colour, mask):\n",
    "    coord = []\n",
    "    for j in range(mask.shape[1]):\n",
    "        for i in range(mask.shape[0]):\n",
    "            if np.array_equal(mask[i][j], colour):\n",
    "                coord.append([i, j])\n",
    "                break\n",
    "    return coord\n",
    "\n",
    "\n",
    "def extract_colored_inner_lining(color_mask):\n",
    "    left_colors = [\n",
    "        [255, 0, 0],     \n",
    "        [0, 255, 0],     # Green\n",
    "        [0, 0, 255],     # Red\n",
    "    ]\n",
    "    right_colors = [\n",
    "        [255, 255, 0],  \n",
    "        [255, 0, 255],   \n",
    "        [0, 255, 255],  \n",
    "    ]\n",
    "\n",
    "    inner_lining_mask = np.zeros_like(color_mask)\n",
    "\n",
    "    for i in left_colors:\n",
    "        coord=traverse_right(i, color_mask)\n",
    "        for x, y in coord:\n",
    "            inner_lining_mask[x, y] = i\n",
    "\n",
    "    for i in right_colors:\n",
    "        coord=traverse_left(i, color_mask)\n",
    "        for x, y in coord:\n",
    "            inner_lining_mask[x, y] = i\n",
    "    return inner_lining_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ff025",
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-08T12:41:16.592Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Feature_extraction(video_path, model, device):\n",
    "    segment_colors = [\n",
    "        [255, 0, 0],     # Blue    - Left segment 1\n",
    "        [0, 255, 0],     # Green   - Left segment 2\n",
    "        [0, 0, 255],     # Red     - Left segment 3\n",
    "        [255, 255, 0],   # Cyan    - Right segment 1\n",
    "        [255, 0, 255],   # Magenta - Right segment 2\n",
    "        [0, 255, 255],   # Yellow  - Right segment 3\n",
    "    ]\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    video_matrix =extract_mask_matrix_from_video(video_path, model, device)\n",
    "    features_area = []\n",
    "    features_boundary = []\n",
    "    features_center = []\n",
    "\n",
    "    area_ref=[]\n",
    "    count1=0\n",
    "    for frame in video_matrix:\n",
    "        print(count1)\n",
    "        count1+=1\n",
    "        # print(\"Original shape:\", frame.shape)\n",
    "    \n",
    "        # Normalize to [0, 255] range if not already\n",
    "        # print(frame.shape)\n",
    "        if(count1>27): \n",
    "            break\n",
    "        if frame.max() <= 1.0:\n",
    "            frame = (frame * 255).astype(np.uint8)\n",
    "\n",
    "        color_mask = color_u_mask(frame)\n",
    "    \n",
    "        # plot_image(color_mask)\n",
    "        points=get_segment_points(color_mask, segment_colors)\n",
    "        if(len(area_ref)==0):\n",
    "            area_ref=points\n",
    "        # print(area_ref)\n",
    "        intersection = set(map(tuple, points)) & set(map(tuple, area_ref))\n",
    "\n",
    "        count = len(intersection)\n",
    "        # print(points)\n",
    "        features_area.append(count/len(area_ref))\n",
    "        \n",
    "        \n",
    "        # break  # Only process one frame for now\n",
    "    \n",
    "    print(features_area)\n",
    "    print(len(features_area))\n",
    "    print(video_matrix.shape)\n",
    "    return video_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87644cf1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6875393,
     "sourceId": 11038255,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7358347,
     "sourceId": 11721787,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 329109,
     "modelInstanceId": 308701,
     "sourceId": 373105,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.213338,
   "end_time": "2025-05-08T14:32:13.731448",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T14:31:54.518110",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
