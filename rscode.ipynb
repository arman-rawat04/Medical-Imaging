{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11038255,"sourceType":"datasetVersion","datasetId":6875393},{"sourceId":373105,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":308701,"modelId":329109}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport cv2\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.102410Z","iopub.execute_input":"2025-05-08T11:54:33.102925Z","iopub.status.idle":"2025-05-08T11:54:33.107012Z","shell.execute_reply.started":"2025-05-08T11:54:33.102904Z","shell.execute_reply":"2025-05-08T11:54:33.106115Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.108159Z","iopub.execute_input":"2025-05-08T11:54:33.108466Z","iopub.status.idle":"2025-05-08T11:54:33.124868Z","shell.execute_reply.started":"2025-05-08T11:54:33.108449Z","shell.execute_reply":"2025-05-08T11:54:33.124348Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Preprocessing_functions","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\n\nimport cv2\nimport numpy as np\n\ndef overlay_mask_on_frame(original_frame, mask, alpha=0.5):\n    if len(mask.shape) == 2:\n        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)  \n    if len(original_frame.shape) == 2:\n        original_frame = cv2.cvtColor(original_frame, cv2.COLOR_GRAY2BGR)  \n\n    mask_resized = cv2.resize(mask, (original_frame.shape[1], original_frame.shape[0]))\n\n    overlayed_image = cv2.addWeighted(original_frame, 1 - alpha, mask_resized, alpha, 0)\n\n    return overlayed_image\n\n\ndef apply_clahe(gray_frame):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    return clahe.apply(gray_frame)\n\n\ndef preprocess(frame, size=(224, 224)):\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    resized = cv2.resize(frame_gray, size)\n    enhanced = apply_clahe(resized)\n    tensor = torch.from_numpy(enhanced).float().unsqueeze(0) / 255.0\n    return tensor.unsqueeze(0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.131503Z","iopub.execute_input":"2025-05-08T11:54:33.131754Z","iopub.status.idle":"2025-05-08T11:54:33.140566Z","shell.execute_reply.started":"2025-05-08T11:54:33.131738Z","shell.execute_reply":"2025-05-08T11:54:33.139888Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# ****Segmenetation","metadata":{}},{"cell_type":"code","source":"\nclass ConvBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n\n        # number of input channels is a number of filters in the previous layer\n        # number of output channels is a number of filters in the current layer\n        # \"same\" convolutions\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass UpConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(UpConv, self).__init__()\n\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"Attention block with learnable parameters\"\"\"\n\n    def __init__(self, F_g, F_l, n_coefficients):\n        \"\"\"\n        :param F_g: number of feature maps (channels) in previous layer\n        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n        \"\"\"\n        super(AttentionBlock, self).__init__()\n\n        self.W_gate = nn.Sequential(\n            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(n_coefficients)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(n_coefficients)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, gate, skip_connection):\n        \"\"\"\n        :param gate: gating signal from previous layer\n        :param skip_connection: activation from corresponding encoder layer\n        :return: output activations\n        \"\"\"\n        g1 = self.W_gate(gate)\n        x1 = self.W_x(skip_connection)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = skip_connection * psi\n        return out\n\n\nclass AttentionUNet(nn.Module):\n\n    def __init__(self, img_ch=1, output_ch=1):  # Changed img_ch to 1\n        super(AttentionUNet, self).__init__()\n\n        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = ConvBlock(img_ch, 64)\n        self.Conv2 = ConvBlock(64, 128)\n        self.Conv3 = ConvBlock(128, 256)\n        self.Conv4 = ConvBlock(256, 512)\n        self.Conv5 = ConvBlock(512, 1024)\n\n        self.Up5 = UpConv(1024, 512)\n        self.Att5 = AttentionBlock(F_g=512, F_l=512, n_coefficients=256)\n        self.UpConv5 = ConvBlock(1024, 512)\n\n        self.Up4 = UpConv(512, 256)\n        self.Att4 = AttentionBlock(F_g=256, F_l=256, n_coefficients=128)\n        self.UpConv4 = ConvBlock(512, 256)\n\n        self.Up3 = UpConv(256, 128)\n        self.Att3 = AttentionBlock(F_g=128, F_l=128, n_coefficients=64)\n        self.UpConv3 = ConvBlock(256, 128)\n\n        self.Up2 = UpConv(128, 64)\n        self.Att2 = AttentionBlock(F_g=64, F_l=64, n_coefficients=32)\n        self.UpConv2 = ConvBlock(128, 64)\n\n        self.Conv = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        e1 = self.Conv1(x)\n\n        e2 = self.MaxPool(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.MaxPool(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.MaxPool(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.MaxPool(e4)\n        e5 = self.Conv5(e5)\n\n        d5 = self.Up5(e5)\n        s4 = self.Att5(gate=d5, skip_connection=e4)\n        d5 = torch.cat((s4, d5), dim=1)\n        d5 = self.UpConv5(d5)\n\n        d4 = self.Up4(d5)\n        s3 = self.Att4(gate=d4, skip_connection=e3)\n        d4 = torch.cat((s3, d4), dim=1)\n        d4 = self.UpConv4(d4)\n\n        d3 = self.Up3(d4)\n        s2 = self.Att3(gate=d3, skip_connection=e2)\n        d3 = torch.cat((s2, d3), dim=1)\n        d3 = self.UpConv3(d3)\n\n        d2 = self.Up2(d3)\n        s1 = self.Att2(gate=d2, skip_connection=e1)\n        d2 = torch.cat((s1, d2), dim=1)\n        d2 = self.UpConv2(d2)\n\n        out = self.Conv(d2)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.220891Z","iopub.execute_input":"2025-05-08T11:54:33.221562Z","iopub.status.idle":"2025-05-08T11:54:33.236591Z","shell.execute_reply.started":"2025-05-08T11:54:33.221537Z","shell.execute_reply":"2025-05-08T11:54:33.235807Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\n# Import the model class (ensure the model code is in scope or imported)\nsegmentation_model = AttentionUNet(img_ch=1, output_ch=1)\n\n\n\nweights_path = '/kaggle/input/seg2/other/default/1/checkpoint_epoch_30.pth'  # Update with correct path\n\nstate_dict = torch.load(weights_path, map_location=device)\n\nsegmentation_model.load_state_dict(state_dict)\nsegmentation_model = segmentation_model.to(device)\n\n# Set to evaluation mode\nsegmentation_model.eval()\n\nprint(\"Segmentation model weights loaded successfully.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.237756Z","iopub.execute_input":"2025-05-08T11:54:33.238499Z","iopub.status.idle":"2025-05-08T11:54:33.646992Z","shell.execute_reply.started":"2025-05-08T11:54:33.238473Z","shell.execute_reply":"2025-05-08T11:54:33.646274Z"}},"outputs":[{"name":"stdout","text":"Segmentation model weights loaded successfully.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3971176255.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(weights_path, map_location=device)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"\ndef extract_mask_matrix_from_video(video_path, model, device, frame_size=(224, 224)):\n    cap = cv2.VideoCapture(video_path)\n    mask_list = []\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame_clahe = apply_clahe(frame)\n\n        input_tensor = preprocess_frame_for_model(frame_clahe, size=frame_size).to(device)\n        with torch.no_grad():\n            pred_mask = model(input_tensor)  # Shape: [1, 1, H, W]\n        binary_mask = (pred_mask.squeeze().cpu().numpy() > 0.5).astype(np.uint8)  # Shape: [H, W]\n        binary_mask=overlay_mask_on_frame(frame, binary_mask)\n        mask_list.append(binary_mask)\n\n    cap.release()\n    mask_matrix = np.stack(mask_list, axis=0)  \n    return mask_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.648039Z","iopub.execute_input":"2025-05-08T11:54:33.648272Z","iopub.status.idle":"2025-05-08T11:54:33.653440Z","shell.execute_reply.started":"2025-05-08T11:54:33.648255Z","shell.execute_reply":"2025-05-08T11:54:33.652852Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature extraction Helper functions","metadata":{}},{"cell_type":"code","source":"def traverse_left(colour, mask):\n    coord = []\n    for i in range(mask.shape[0]):\n        for j in range(mask.shape[1]):\n            if np.array_equal(mask[i][j], colour):\n                coord.append([i, j])\n                break\n    return coord\n\ndef traverse_right(colour, mask):\n    coord = []\n    for i in range(mask.shape[0]):\n        for j in range(mask.shape[1]-1, -1, -1):\n            if np.array_equal(mask[i][j], colour):\n                coord.append([i, j])\n                break\n    return coord\n\ndef traverse_up(colour, mask):\n    coord = []\n    for j in range(mask.shape[1]):\n        for i in range(mask.shape[0]):\n            if np.array_equal(mask[i][j], colour):\n                coord.append([i, j])\n                break\n    return coord\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.654228Z","iopub.execute_input":"2025-05-08T11:54:33.654454Z","iopub.status.idle":"2025-05-08T11:54:33.668462Z","shell.execute_reply.started":"2025-05-08T11:54:33.654435Z","shell.execute_reply":"2025-05-08T11:54:33.667905Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef color_segments_with_skip(coords_sorted, color_mask, colors):\n    n_segments = len(colors)\n    total_length = 0\n    distances = [0]\n\n    for i in range(1, len(coords_sorted)):\n        prev = coords_sorted[i - 1]\n        curr = coords_sorted[i]\n        total_length += np.linalg.norm(curr - prev)\n        distances.append(total_length)\n\n    skip_len = 1.5 * total_length / 7\n    usable_len = total_length - skip_len\n    segment_len = usable_len / n_segments\n\n    start_idx = 0\n    while start_idx < len(distances) and distances[start_idx] < skip_len:\n        start_idx += 1\n\n    segments = [[] for _ in range(n_segments)]\n    current_length = distances[start_idx]\n    i = start_idx\n    while i < len(distances) and current_length < total_length:\n        segment_index = int((current_length - skip_len) // segment_len)\n        if segment_index < n_segments:\n            segments[segment_index].append(coords_sorted[i])\n        current_length = distances[i]\n        i += 1\n\n    for seg_id, segment in enumerate(segments):\n        for x, y in segment:\n            color_mask[y, x] = colors[seg_id]\n\ndef get_segment_midpoints(color_mask, segment_colors):\n    midpoints = []\n\n    for color in segment_colors:\n        color_arr = np.array(color, dtype=np.uint8)\n        color_pixels = np.all(color_mask == color_arr, axis=-1)\n\n        ys, xs = np.where(color_pixels)\n        if len(xs) == 0 or len(ys) == 0:\n            midpoints.append(None)\n            continue\n\n        x_mean = int(np.mean(xs))\n        y_mean = int(np.mean(ys))\n        midpoints.append((x_mean, y_mean))\n\n    return midpoints\n\ndef get_segment_areas(color_mask, segment_colors):\n    areas = []\n\n    for color in segment_colors:\n        color_arr = np.array(color, dtype=np.uint8)\n        color_pixels = np.all(color_mask == color_arr, axis=-1)\n\n        ys, xs = np.where(color_pixels)\n        area = len(xs)  # Area is the number of pixels of this color\n        areas.append(area)\n\n    return areas\n\n\ndef extract_colored_inner_lining(color_mask):\n    left_colors = [\n        [255, 0, 0],     \n        [0, 255, 0],     # Green\n        [0, 0, 255],     # Red\n    ]\n    right_colors = [\n        [255, 255, 0],  \n        [255, 0, 255],   \n        [0, 255, 255],  \n    ]\n\n    inner_lining_mask = np.zeros_like(color_mask)\n\n    for i in left_colors:\n        coord=traverse_right(i, color_mask)\n        for x, y in coord:\n            inner_lining_mask[x, y] = i\n\n    for i in right_colors:\n        coord=traverse_left(i, color_mask)\n        for x, y in coord:\n            inner_lining_mask[x, y] = i\n    return inner_lining_mask\n\ndef color_u_mask(mask):\n    ys, xs = np.where(mask == 255)\n    coords = np.column_stack((xs, ys))\n\n    topmost_index = np.argmin(ys)\n    x_center = xs[topmost_index]\n\n    left_arm = coords[coords[:, 0] < x_center]\n    right_arm = coords[coords[:, 0] > x_center]\n\n    left_sorted = left_arm[np.argsort(left_arm[:, 1])]\n    right_sorted = right_arm[np.argsort(right_arm[:, 1])]\n\n    left_colors = [\n        [255, 0, 0],     # Blue\n        [0, 255, 0],     # Green\n        [0, 0, 255],     # Red\n    ]\n    right_colors = [\n        [255, 255, 0],   # Cyan\n        [255, 0, 255],   # Magenta\n        [0, 255, 255],   # Yellow\n    ]\n\n    color_mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n\n    color_segments_with_skip(left_sorted, color_mask, left_colors)\n    color_segments_with_skip(right_sorted, color_mask, right_colors)\n\n\n    \n   \n    return color_mask\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.670050Z","iopub.execute_input":"2025-05-08T11:54:33.670253Z","iopub.status.idle":"2025-05-08T11:54:33.685304Z","shell.execute_reply.started":"2025-05-08T11:54:33.670240Z","shell.execute_reply":"2025-05-08T11:54:33.684654Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"def get_segment_points(color_mask, segment_colors):\n    segment_points = []\n\n    for color in segment_colors:\n        print(color)\n        for i in range(color_mask.shape[0]):\n            for j in range(color_mask.shape[1]):\n                if np.array_equal(color_mask[i][j], color):\n                    segment_points.append([i, j])\n                    \n        \n    return segment_points\n\ndef Feature_extraction(video_path):\n    segment_colors = [\n        [255, 0, 0],     # Blue    - Left segment 1\n        [0, 255, 0],     # Green   - Left segment 2\n        [0, 0, 255],     # Red     - Left segment 3\n        [255, 255, 0],   # Cyan    - Right segment 1\n        [255, 0, 255],   # Magenta - Right segment 2\n        [0, 255, 255],   # Yellow  - Right segment 3\n    ]\n\n\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Could not open video file: {video_path}\")\n\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n\n    cap.release()\n\n    if not frames:\n        raise ValueError(\"No frames were read from the video.\")\n\n    video_matrix = np.stack(frames, axis=0)\n    features_area=[]\n    features_boundary=[]\n    features_center=[]\n\n    area_ref=[]\n    for frame in video_matrix:\n        preprocessed = preprocess(frame)  \n        mask = segmentation_model(preprocessed) \n        plot_image(mask)\n        # mask_np = mask.squeeze().numpy().astype(np.uint8)  \n        mask_np = mask.squeeze().detach().cpu().numpy().astype(np.uint8)\n\n    \n        color_mask = color_u_mask(mask_np)  \n    \n        centers = get_segment_points(color_mask, segment_colors)\n        print(centers)\n        break\n        # areas = get_segment_pixels(color_mask, segment_colors)\n        # boundary = get_inner_boundary(color_mask, segment_colors)\n    \n        # features_center.append(centers)\n        # features_area.append(areas)\n        # features_boundary.append(boundary)\n\n    print(video_matrix.shape)\n    return video_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.685840Z","iopub.execute_input":"2025-05-08T11:54:33.686070Z","iopub.status.idle":"2025-05-08T11:54:33.704630Z","shell.execute_reply.started":"2025-05-08T11:54:33.686055Z","shell.execute_reply":"2025-05-08T11:54:33.703937Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# video_path = \"/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A4C/ES0001 _4CH_1.avi\"\n# video_matrix = Feature_extraction(video_path)\n\n# print(\"Video shape:\", video_matrix.shape)  # (num_frames, height, width, 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.705297Z","iopub.execute_input":"2025-05-08T11:54:33.705530Z","iopub.status.idle":"2025-05-08T11:54:33.722745Z","shell.execute_reply.started":"2025-05-08T11:54:33.705512Z","shell.execute_reply":"2025-05-08T11:54:33.722047Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_image(image):\n    # If the image is in grayscale (single channel), it will be a 2D array\n    if len(image.shape) == 2:\n        plt.imshow(image, cmap='gray')\n    else:\n        plt.imshow(image)  # Color image (3 channels)\n\n    plt.axis('off')  # Turn off axis labels\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.723417Z","iopub.execute_input":"2025-05-08T11:54:33.723675Z","iopub.status.idle":"2025-05-08T11:54:33.737482Z","shell.execute_reply.started":"2025-05-08T11:54:33.723652Z","shell.execute_reply":"2025-05-08T11:54:33.736789Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\ndef plot_image(image):\n    # Ensure that the image is a NumPy array\n    # if isinstance(image, torch.Tensor):\n    #     image = image.detach().cpu().numpy()\n    if frame.ndim == 3 and frame.shape[2] == 1:\n        frame = frame.squeeze(axis=-1)  # Remove the channel dimension if it's just a single channel (grayscale)\n    plot_image(frame)  # Plot the mask to visualize\n\n    # If the image is in grayscale (single channel), it will be a 2D array\n    if len(image.shape) == 2:\n        plt.imshow(image, cmap='gray')\n    else:\n        plt.imshow(image)  # Color image (3 channels)\n\n    plt.axis('off')  # Turn off axis labels\n    plt.show()\n\ndef get_segment_points(color_mask, segment_colors):\n    segment_points = []\n\n    for color in segment_colors:\n        print(f\"Checking for color: {color}\")\n        color = np.array(color, dtype=np.uint8)\n        for i in range(color_mask.shape[0]):\n            for j in range(color_mask.shape[1]):\n                if np.array_equal(color_mask[i][j], color):  # Check if the pixel matches the segment color\n                    segment_points.append([i, j])\n                    \n    return segment_points\ndef apply_clahe(frame):\n    \"\"\"Apply CLAHE to enhance contrast on a BGR frame.\"\"\"\n    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n\n    merged = cv2.merge((cl, a, b))\n    enhanced = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n    return enhanced\n\ndef preprocess_frame_for_model(frame, size=(224, 224)):\n    \"\"\"Resize and convert a frame to grayscale for model input.\"\"\"\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    resized = cv2.resize(frame_gray, size)  # Shape: [H, W]\n    tensor = torch.from_numpy(resized).float().unsqueeze(0) / 255.0  # Shape: [1, H, W]\n    return tensor.unsqueeze(0)  # Shape: [1, 1, H, W]\n\n# def preprocess(frame, size=(224, 224)):\n#     # Preprocessing the frame (grayscale and resizing)\n#     frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n#     resized = cv2.resize(frame_gray, size)\n#     return resized\n\ndef segmentation_model(preprocessed_frame):\n    # Dummy segmentation model, replace with your actual model\n    # Assuming the model returns a 3D tensor (height, width, channels)\n    return torch.rand((preprocessed_frame.shape[0], preprocessed_frame.shape[1], 3))\n\ndef extract_mask_matrix_from_video(video_path, model, device, frame_size=(224, 224)):\n    cap = cv2.VideoCapture(video_path)\n    mask_list = []\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame_clahe = apply_clahe(frame)\n\n        input_tensor = preprocess_frame_for_model(frame_clahe, size=frame_size).to(device)\n        print(input_tensor.shape)\n        with torch.no_grad():\n            pred_mask = model(input_tensor)  # Shape: [1, 1, H, W]\n            \n        print(pred_mask.shape)\n\n        binary_mask = (pred_mask.squeeze().cpu().numpy() > 0.5).astype(np.uint8)  # Shape: [H, W]\n        # binary_mask=overlay_mask_on_frame(frame, binary_mask)\n        mask_list.append(binary_mask)\n\n    cap.release()\n\n    # Stack all binary masks into a 3D numpy array (frames, height, width)\n    mask_matrix = np.stack(mask_list, axis=0)  # Shape: [num_frames, H, W]\n    return mask_matrix\n\ndef Feature_extraction(video_path, model, device):\n    segment_colors = [\n        [255, 0, 0],     # Blue    - Left segment 1\n        [0, 255, 0],     # Green   - Left segment 2\n        [0, 0, 255],     # Red     - Left segment 3\n        [255, 255, 0],   # Cyan    - Right segment 1\n        [255, 0, 255],   # Magenta - Right segment 2\n        [0, 255, 255],   # Yellow  - Right segment 3\n    ]\n\n\n   \n\n    video_matrix =extract_mask_matrix_from_video(video_path, model, device)\n    features_area = []\n    features_boundary = []\n    features_center = []\n\n    for frame in video_matrix:\n        \n        \n       \n        plot_image(frame)  # Plot the mask to visualize\n        break\n\n        # Assuming color_u_mask is a function to map the mask to color segments\n        # color_mask = mask_np  # Use this directly if it is already color-mapped\n        \n        # centers = get_segment_points(color_mask, segment_colors)\n        # print(centers)\n\n        # # Store extracted features\n        # features_center.append(centers)\n        # # features_area.append(areas)  # If you have area extraction logic\n        # # features_boundary.append(boundary)  # If you have boundary extraction logic\n        # break\n\n    print(video_matrix.shape)\n    return video_matrix\n\nvideo_path = \"/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A4C/ES0001 _4CH_1.avi\"\nvideo_matrix = Feature_extraction(video_path, segmentation_model, device)\n\nprint(\"Video shape:\", video_matrix.shape)  # (num_frames, height, width, 3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.738250Z","iopub.execute_input":"2025-05-08T11:54:33.738471Z","iopub.status.idle":"2025-05-08T11:54:33.994220Z","shell.execute_reply.started":"2025-05-08T11:54:33.738451Z","shell.execute_reply":"2025-05-08T11:54:33.993440Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\ntorch.Size([1, 1, 224, 224])\ntorch.Size([1, 1, 3])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1650321274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A4C/ES0001 _4CH_1.avi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mvideo_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (num_frames, height, width, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1650321274.py\u001b[0m in \u001b[0;36mFeature_extraction\u001b[0;34m(video_path, model, device)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Plot the mask to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1650321274.py\u001b[0m in \u001b[0;36mplot_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# if isinstance(image, torch.Tensor):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#     image = image.detach().cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove the channel dimension if it's just a single channel (grayscale)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Plot the mask to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'frame' where it is not associated with a value"],"ename":"UnboundLocalError","evalue":"cannot access local variable 'frame' where it is not associated with a value","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"def Feature_extraction(video_path):\n    segment_colors = [\n        [255, 0, 0],     # Blue    - Left segment 1\n        [0, 255, 0],     # Green   - Left segment 2\n        [0, 0, 255],     # Red     - Left segment 3\n        [255, 255, 0],   # Cyan    - Right segment 1\n        [255, 0, 255],   # Magenta - Right segment 2\n        [0, 255, 255],   # Yellow  - Right segment 3\n    ]\n\n    cap = cv2.VideoCapture(video_path)\n    mask_list = []\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Step 1: Enhance contrast using CLAHE\n        frame_clahe = preprocess(frame)\n\n        # Step 2: Preprocess and move to device\n        input_tensor = torch.tensor(frame_clahe, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n\n        # Step 3: Get prediction from segmentation model\n        with torch.no_grad():\n            pred_mask = segmentation_model(input_tensor)  # Shape: [1, 1, H, W]\n\n        # Step 4: Convert to binary mask\n        binary_mask = (pred_mask.squeeze().cpu().numpy() > 0.5).astype(np.uint8)  # Shape: [H, W]\n        mask_list.append(binary_mask)\n\n        # Check the dimensions and plot the image (inside the loop)\n        if binary_mask.ndim == 3 and binary_mask.shape[2] == 1:\n            binary_mask = binary_mask.squeeze(axis=-1)  # Remove the channel dimension if it's just a single channel (grayscale)\n\n        plot_image(binary_mask)  # Plot the mask to visualize\n\n    cap.release()\n\n    video_matrix = np.stack(mask_list, axis=0)\n    print(video_matrix.shape)\n    return video_matrix\n\nvideo_path = \"/kaggle/input/medimgproject/HMC-QU Dataset-Kaggle/HMC-QU/A4C/ES0001 _4CH_1.avi\"\nvideo_matrix = Feature_extraction(video_path)\n\nprint(\"Video shape:\", video_matrix.shape)  # (num_frames, height, width, 3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T11:54:33.994760Z","iopub.status.idle":"2025-05-08T11:54:33.994992Z","shell.execute_reply.started":"2025-05-08T11:54:33.994887Z","shell.execute_reply":"2025-05-08T11:54:33.994897Z"}},"outputs":[],"execution_count":null}]}