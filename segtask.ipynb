{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11671891,"sourceType":"datasetVersion","datasetId":7325100},{"sourceId":373070,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":308672,"modelId":329080}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torchvision.transforms as transforms\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy as np\nimport cv2\nimport os\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch.optim as optim\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:40.957727Z","iopub.execute_input":"2025-05-04T18:46:40.958428Z","iopub.status.idle":"2025-05-04T18:46:40.962780Z","shell.execute_reply.started":"2025-05-04T18:46:40.958400Z","shell.execute_reply":"2025-05-04T18:46:40.962027Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:43.503287Z","iopub.execute_input":"2025-05-04T18:46:43.503953Z","iopub.status.idle":"2025-05-04T18:46:43.508186Z","shell.execute_reply.started":"2025-05-04T18:46:43.503926Z","shell.execute_reply":"2025-05-04T18:46:43.507455Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:45.353165Z","iopub.execute_input":"2025-05-04T18:46:45.353850Z","iopub.status.idle":"2025-05-04T18:46:45.357118Z","shell.execute_reply.started":"2025-05-04T18:46:45.353824Z","shell.execute_reply":"2025-05-04T18:46:45.356546Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        \"\"\"\n        Args:\n            image_dir (str): Path to frames directory.\n            mask_dir (str): Path to masks directory.\n            transform (albumentations.Compose, optional): Transform to apply.\n        \"\"\"\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n\n        # Sort the files by name to ensure they match up correctly\n        self.images = sorted(os.listdir(image_dir))  # Sorting image filenames\n        self.masks = sorted(os.listdir(mask_dir))    # Sorting mask filenames\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.images[idx])\n        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n\n        # Load both in grayscale\n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        # Convert to float32 and normalize to [0, 1]\n        image = image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n        mask = mask.astype(np.float32) / 255.0    # Normalize to [0, 1]\n        \n        # Expand dimensions to (H, W, 1) for albumentations\n        image = np.expand_dims(image, axis=-1)  # (H, W, 1)\n        mask = np.expand_dims(mask, axis=-1)    # (H, W, 1)\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented[\"image\"]  # Tensor: (1, 224, 224)\n            augmented = self.transform(image=mask)\n            mask = augmented[\"image\"]    # Tensor: (224, 224)\n\n        # Ensure mask has the shape (1, 224, 224) for consistency\n        if len(mask.shape) == 2:  # If mask is (224, 224)\n            mask = mask.unsqueeze(0)  # Convert to (1, 224, 224)\n\n        return image, mask  # Shape: (1, 224, 224), (1, 224, 224)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:46.759398Z","iopub.execute_input":"2025-05-04T18:46:46.760093Z","iopub.status.idle":"2025-05-04T18:46:46.766666Z","shell.execute_reply.started":"2025-05-04T18:46:46.760071Z","shell.execute_reply":"2025-05-04T18:46:46.765718Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"image_dir = \"/kaggle/input/segmentation/data1/encoder_directory_new\"  \nmask_dir = \"/kaggle/input/segmentation/data1/decoder_directory_new\"    \n\ntransform = A.Compose([\n    A.Resize(224, 224),\n    ToTensorV2()  \n])\n\ndataset = SegmentationDataset(image_dir=image_dir, mask_dir=mask_dir, transform=transform)\n\nprint(f\"Dataset size: {len(dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:51.070294Z","iopub.execute_input":"2025-05-04T18:46:51.070550Z","iopub.status.idle":"2025-05-04T18:46:51.091753Z","shell.execute_reply.started":"2025-05-04T18:46:51.070533Z","shell.execute_reply":"2025-05-04T18:46:51.091014Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 3526\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import random_split, DataLoader\n\n# Define the splits\ntrain_size = int(0.75 * len(dataset))  # 75% for training\nval_size = int(0.10 * len(dataset))   # 10% for validation\ntest_size = len(dataset) - train_size - val_size  # 15% for testing\n\n# Perform the split\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Example to check the data loader sizes\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:53.586498Z","iopub.execute_input":"2025-05-04T18:46:53.586967Z","iopub.status.idle":"2025-05-04T18:46:53.610906Z","shell.execute_reply.started":"2025-05-04T18:46:53.586943Z","shell.execute_reply":"2025-05-04T18:46:53.610213Z"}},"outputs":[{"name":"stdout","text":"Training set size: 2644\nValidation set size: 352\nTest set size: 530\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:46:57.793554Z","iopub.execute_input":"2025-05-04T18:46:57.794106Z","iopub.status.idle":"2025-05-04T18:46:57.810274Z","shell.execute_reply.started":"2025-05-04T18:46:57.794085Z","shell.execute_reply":"2025-05-04T18:46:57.809533Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Import the model class (ensure the model code is in scope or imported)\nsegmentation_model = AttentionUNet(img_ch=1, output_ch=1)\n\n\n\nweights_path = '/kaggle/input/seg1/other/default/1/checkpoint_epoch_20.pth'  # Update with correct path\n\nstate_dict = torch.load(weights_path, map_location=device)\n\nsegmentation_model.load_state_dict(state_dict)\n\n# Set to evaluation mode\nsegmentation_model.eval()\n\nprint(\"Segmentation model weights loaded successfully.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, alpha=0.7, beta=0.3, gamma=0.75, smooth=1e-6):\n        \"\"\"\n        alpha: controls penalty for false positives\n        beta: controls penalty for false negatives\n        gamma: focusing parameter to focus on hard examples\n        smooth: smoothing constant to avoid division by zero\n        \"\"\"\n        super(FocalTverskyLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.smooth = smooth\n\n    def forward(self, preds, targets):\n        \"\"\"\n        preds: predicted mask (logits or probabilities) - shape [B, 1, H, W]\n        targets: ground truth mask - shape [B, 1, H, W]\n        \"\"\"\n        # Apply sigmoid if preds are logits\n        preds = torch.sigmoid(preds)\n\n        # Flatten the tensors\n        preds = preds.view(-1)\n        targets = targets.view(-1)\n\n        # Calculate Tversky components\n        TP = (preds * targets).sum()\n        FP = ((1 - targets) * preds).sum()\n        FN = (targets * (1 - preds)).sum()\n\n        # Tversky index\n        Tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n\n        # Focal Tversky Loss\n        loss = (1 - Tversky) ** self.gamma\n\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:47:05.488463Z","iopub.execute_input":"2025-05-04T18:47:05.489193Z","iopub.status.idle":"2025-05-04T18:47:05.495248Z","shell.execute_reply.started":"2025-05-04T18:47:05.489166Z","shell.execute_reply":"2025-05-04T18:47:05.494536Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"criterion = FocalTverskyLoss(alpha=0.7, beta=0.3, gamma=0.75)  # Your custom loss function\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:47:09.046119Z","iopub.execute_input":"2025-05-04T18:47:09.046433Z","iopub.status.idle":"2025-05-04T18:47:09.051221Z","shell.execute_reply.started":"2025-05-04T18:47:09.046412Z","shell.execute_reply":"2025-05-04T18:47:09.050501Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n\n# Training Loop\nnum_epochs = 40\nprint(\"starting\")\nfor epoch in range(num_epochs):\n    print(f\"starting {epoch}\")\n    model.train()  # Set the model to training mode\n    running_loss = 0.0\n    \n    for batch_idx, (images, masks) in enumerate(train_loader):  # Loading batches of images and masks\n        images, masks = images.to(device), masks.to(device)  # Move to device (GPU/CPU)\n        \n        optimizer.zero_grad()  # Clear gradients from the previous step\n        \n        # Forward pass: Get predictions\n        outputs = model(images)\n        \n        # Calculate loss\n        loss = criterion(outputs, masks)\n        \n        # Backward pass: Calculate gradients\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    avg_train_loss = running_loss / len(train_loader)  # Average loss for this epoch\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n    \n    # Validation after each epoch\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # No gradient calculation during validation\n        val_loss = 0.0\n        for images, masks in val_loader:  # Iterate through validation data\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n    \n    # Save model checkpoint after every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        checkpoint_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n    \n    # Optionally, you can save the final model at the end of training\n    if (epoch + 1) == num_epochs:\n        final_model_path = \"final_model.pth\"\n        torch.save(model.state_dict(), final_model_path)\n        print(f\"Final model saved at epoch {epoch + 1}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:47:11.266042Z","iopub.execute_input":"2025-05-04T18:47:11.266305Z","iopub.status.idle":"2025-05-04T19:48:50.347588Z","shell.execute_reply.started":"2025-05-04T18:47:11.266288Z","shell.execute_reply":"2025-05-04T19:48:50.346952Z"}},"outputs":[{"name":"stdout","text":"starting\nstarting 0\nEpoch [1/40], Loss: 0.8650\nValidation Loss: 0.8443\nstarting 1\nEpoch [2/40], Loss: 0.8314\nValidation Loss: 0.8185\nstarting 2\nEpoch [3/40], Loss: 0.8066\nValidation Loss: 0.7973\nstarting 3\nEpoch [4/40], Loss: 0.7766\nValidation Loss: 0.7586\nstarting 4\nEpoch [5/40], Loss: 0.7403\nValidation Loss: 0.7225\nstarting 5\nEpoch [6/40], Loss: 0.6963\nValidation Loss: 0.6779\nstarting 6\nEpoch [7/40], Loss: 0.6453\nValidation Loss: 0.6147\nstarting 7\nEpoch [8/40], Loss: 0.5895\nValidation Loss: 0.5554\nstarting 8\nEpoch [9/40], Loss: 0.5308\nValidation Loss: 0.5015\nstarting 9\nEpoch [10/40], Loss: 0.4697\nValidation Loss: 0.4346\nCheckpoint saved at epoch 10\nstarting 10\nEpoch [11/40], Loss: 0.4123\nValidation Loss: 0.3903\nstarting 11\nEpoch [12/40], Loss: 0.3616\nValidation Loss: 0.3367\nstarting 12\nEpoch [13/40], Loss: 0.3177\nValidation Loss: 0.3070\nstarting 13\nEpoch [14/40], Loss: 0.2787\nValidation Loss: 0.2923\nstarting 14\nEpoch [15/40], Loss: 0.2471\nValidation Loss: 0.2459\nstarting 15\nEpoch [16/40], Loss: 0.2193\nValidation Loss: 0.2135\nstarting 16\nEpoch [17/40], Loss: 0.1974\nValidation Loss: 0.1930\nstarting 17\nEpoch [18/40], Loss: 0.1782\nValidation Loss: 0.1762\nstarting 18\nEpoch [19/40], Loss: 0.1619\nValidation Loss: 0.1692\nstarting 19\nEpoch [20/40], Loss: 0.1465\nValidation Loss: 0.1504\nCheckpoint saved at epoch 20\nstarting 20\nEpoch [21/40], Loss: 0.1370\nValidation Loss: 0.1404\nstarting 21\nEpoch [22/40], Loss: 0.1274\nValidation Loss: 0.1349\nstarting 22\nEpoch [23/40], Loss: 0.1186\nValidation Loss: 0.1267\nstarting 23\nEpoch [24/40], Loss: 0.1106\nValidation Loss: 0.1251\nstarting 24\nEpoch [25/40], Loss: 0.1050\nValidation Loss: 0.1172\nstarting 25\nEpoch [26/40], Loss: 0.0991\nValidation Loss: 0.1129\nstarting 26\nEpoch [27/40], Loss: 0.0939\nValidation Loss: 0.1140\nstarting 27\nEpoch [28/40], Loss: 0.0901\nValidation Loss: 0.1002\nstarting 28\nEpoch [29/40], Loss: 0.0868\nValidation Loss: 0.1038\nstarting 29\nEpoch [30/40], Loss: 0.0834\nValidation Loss: 0.0974\nCheckpoint saved at epoch 30\nstarting 30\nEpoch [31/40], Loss: 0.0808\nValidation Loss: 0.0920\nstarting 31\nEpoch [32/40], Loss: 0.0787\nValidation Loss: 0.1050\nstarting 32\nEpoch [33/40], Loss: 0.0755\nValidation Loss: 0.0885\nstarting 33\nEpoch [34/40], Loss: 0.0720\nValidation Loss: 0.0877\nstarting 34\nEpoch [35/40], Loss: 0.0708\nValidation Loss: 0.0899\nstarting 35\nEpoch [36/40], Loss: 0.0698\nValidation Loss: 0.0909\nstarting 36\nEpoch [37/40], Loss: 0.0680\nValidation Loss: 0.0840\nstarting 37\nEpoch [38/40], Loss: 0.0669\nValidation Loss: 0.0897\nstarting 38\nEpoch [39/40], Loss: 0.0640\nValidation Loss: 0.0827\nstarting 39\nEpoch [40/40], Loss: 0.0621\nValidation Loss: 0.0800\nCheckpoint saved at epoch 40\nFinal model saved at epoch 40\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\n# 1. Load the trained model\nmodel = AttentionUNet(img_ch=1, output_ch=1)\nmodel.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_30.pth\", map_location=torch.device('cpu')))\nmodel.eval()\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:56:09.340859Z","iopub.execute_input":"2025-05-04T19:56:09.341129Z","iopub.status.idle":"2025-05-04T19:56:09.682809Z","shell.execute_reply.started":"2025-05-04T19:56:09.341110Z","shell.execute_reply":"2025-05-04T19:56:09.682064Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2227466841.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_30.pth\", map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"AttentionUNet(\n  (MaxPool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv1): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv2): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv3): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv4): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv5): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up5): UpConv(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='nearest')\n      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU(inplace=True)\n    )\n  )\n  (Att5): AttentionBlock(\n    (W_gate): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (UpConv5): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up4): UpConv(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='nearest')\n      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU(inplace=True)\n    )\n  )\n  (Att4): AttentionBlock(\n    (W_gate): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (UpConv4): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up3): UpConv(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='nearest')\n      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU(inplace=True)\n    )\n  )\n  (Att3): AttentionBlock(\n    (W_gate): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (UpConv3): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up2): UpConv(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='nearest')\n      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU(inplace=True)\n    )\n  )\n  (Att2): AttentionBlock(\n    (W_gate): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (UpConv2): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import torch\n\ndef calculate_metrics(model, test_loader, device=\"cuda\"):\n    model.eval()\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    total_pixels = 0\n    correct_pixels = 0\n    \n    with torch.no_grad():\n        for images, masks in test_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs = model(images)\n            preds = torch.sigmoid(outputs) > 0.5  # Binary thresholding\n            \n            # Flatten the predictions and the ground truth masks for comparison\n            preds_flat = preds.view(-1).bool()\n            masks_flat = masks.view(-1).bool()\n            \n            # Calculate TP, FP, FN\n            true_positives += (preds_flat & masks_flat).sum().item()\n            false_positives += ((preds_flat) & (~masks_flat)).sum().item()\n            false_negatives += ((~preds_flat) & (masks_flat)).sum().item()\n            correct_pixels += (preds_flat == masks_flat).sum().item()\n            total_pixels += masks_flat.size(0)\n    \n    # Calculating all metrics\n    dice_coefficient = 2 * true_positives / (2 * true_positives + false_positives + false_negatives) if (2 * true_positives + false_positives + false_negatives) != 0 else 0\n    iou = true_positives / (true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) != 0 else 0\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n    accuracy = correct_pixels / total_pixels * 100 if total_pixels != 0 else 0\n\n    # Print all metrics\n    print(f\"Dice Coefficient: {dice_coefficient:.4f}\")\n    print(f\"IoU: {iou:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall/Sensitivity: {recall:.4f}\")\n    print(f\"Pixel-wise Accuracy: {accuracy:.2f}%\")\n\n# Usage\ncalculate_metrics(model, test_loader, device=\"cuda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:00:05.181373Z","iopub.execute_input":"2025-05-04T20:00:05.182063Z","iopub.status.idle":"2025-05-04T20:00:12.806558Z","shell.execute_reply.started":"2025-05-04T20:00:05.182038Z","shell.execute_reply":"2025-05-04T20:00:12.805807Z"}},"outputs":[{"name":"stdout","text":"Dice Coefficient: 0.9574\nIoU: 0.9184\nPrecision: 0.9835\nRecall/Sensitivity: 0.9327\nPixel-wise Accuracy: 99.65%\n","output_type":"stream"}],"execution_count":29}]}